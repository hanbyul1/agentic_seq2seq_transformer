{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0eb103-23f2-4486-a90c-733bed5a497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Load SWE-bench…\n",
      "[Data] 1024 supervised pairs\n",
      "[Info] Train: 819 pairs, Test: 205 pairs\n",
      "[Agentic][Training] Stage 1: Interleaved ISSUE↔CODE (same epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403226260/work/aten/src/ATen/NestedTensorImpl.cpp:179.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1494\u001B[0m\n\u001B[1;32m   1490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, data, (ids, X, Y, P)\n\u001B[1;32m   1493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1494\u001B[0m     model, data, tensors \u001B[38;5;241m=\u001B[39m \u001B[43mrun_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCFG\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 1344\u001B[0m, in \u001B[0;36mrun_all\u001B[0;34m(cfg)\u001B[0m\n\u001B[1;32m   1342\u001B[0m \u001B[38;5;66;03m# ===== Stage 1: Interleaved ISSUE↔CODE =====\u001B[39;00m\n\u001B[1;32m   1343\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Agentic][Training] Stage 1: Interleaved ISSUE↔CODE (same epoch)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1344\u001B[0m \u001B[43mtrain_stage1_interleaved\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mP_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtok\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1347\u001B[0m \u001B[43m    \u001B[49m\u001B[43missue_max_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_out_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipe_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipe_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipe_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1352\u001B[0m \u001B[43m    \u001B[49m\u001B[43munfreeze_backbone\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1353\u001B[0m \u001B[43m    \u001B[49m\u001B[43munfreeze_adapters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mft_unfreeze_adapters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1354\u001B[0m \u001B[43m    \u001B[49m\u001B[43munfreeze_dec_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mft_unfreeze_dec_norms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_in_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_in_len\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;66;03m# ------ Pipeline Lift (teacher-forced) ------\u001B[39;00m\n\u001B[1;32m   1359\u001B[0m eval_pipeline_lift(\n\u001B[1;32m   1360\u001B[0m     model, data\u001B[38;5;241m.\u001B[39mtok, X_test, Y_test,\n\u001B[1;32m   1361\u001B[0m     issue_max_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmin\u001B[39m(cfg\u001B[38;5;241m.\u001B[39mmax_out_len, \u001B[38;5;241m256\u001B[39m),\n\u001B[1;32m   1362\u001B[0m     max_in_len\u001B[38;5;241m=\u001B[39mcfg\u001B[38;5;241m.\u001B[39mmax_in_len,\n\u001B[1;32m   1363\u001B[0m     device\u001B[38;5;241m=\u001B[39mDEVICE\n\u001B[1;32m   1364\u001B[0m )\n",
      "Cell \u001B[0;32mIn[1], line 919\u001B[0m, in \u001B[0;36mtrain_stage1_interleaved\u001B[0;34m(model, X_train, Y_train, P_train, tok, issue_max_len, epochs, batch_size, lr, device, unfreeze_backbone, unfreeze_adapters, unfreeze_dec_norms, max_in_len)\u001B[0m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;66;03m# Generate issue context (greedy + fallback)\u001B[39;00m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 919\u001B[0m     issue_ctx, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_issue_ctx_greedy_with_fallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtok\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43missue_max_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43missue_max_len\u001B[49m\n\u001B[1;32m    921\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    922\u001B[0m     xb_gist \u001B[38;5;241m=\u001B[39m issue_ctx[:, :max_in_len]  \u001B[38;5;66;03m# ← with gist input\u001B[39;00m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;66;03m# (2) CODE supervised on augmented input\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 1205\u001B[0m, in \u001B[0;36m_issue_ctx_greedy_with_fallback\u001B[0;34m(model, tok, X, issue_max_len)\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1200\u001B[0m \u001B[38;5;124;03mReturns:\u001B[39;00m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;124;03m  issue_ctx  : [B, Tctx] token ids for <ISSUE_GIST>...</ISSUE_GIST> (no BOS/EOS)\u001B[39;00m\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;124;03m  display_ids: [B, Tdisp] BOS ... EOS ids of the plain gist for printing\u001B[39;00m\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1204\u001B[0m \u001B[38;5;66;03m# 1) Greedy generation (no sampling) from the Issue agent\u001B[39;00m\n\u001B[0;32m-> 1205\u001B[0m gen_ids \u001B[38;5;241m=\u001B[39m \u001B[43m_generate_static\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAGENT_ISSUE_ANALYSIS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43missue_max_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_repeat_ngram_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m24\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Increased min_len\u001B[39;49;00m\n\u001B[1;32m   1208\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m    \n\u001B[1;32m   1210\u001B[0m B \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   1211\u001B[0m gists: List[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 430\u001B[0m, in \u001B[0;36m_generate_static\u001B[0;34m(model, X, agent_id, max_len, top_k, top_p, temperature, no_repeat_ngram_size, min_len)\u001B[0m\n\u001B[1;32m    427\u001B[0m ys \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((B, \u001B[38;5;241m1\u001B[39m), BOS, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mX\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, max_len):\n\u001B[0;32m--> 430\u001B[0m     dec \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_states\u001B[49m\u001B[43m(\u001B[49m\u001B[43mys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m     step_logits \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mrouting\u001B[38;5;241m.\u001B[39magents[agent_id]\u001B[38;5;241m.\u001B[39mproject(dec[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:], head\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;66;03m# Block EOS until min_len is reached\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[1], line 399\u001B[0m, in \u001B[0;36mAgenticTransformerSeq2Seq.decode_states\u001B[0;34m(self, y_in, memory, src_key_padding_mask)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecode_states\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_in: torch\u001B[38;5;241m.\u001B[39mTensor, memory: torch\u001B[38;5;241m.\u001B[39mTensor, src_key_padding_mask: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[1], line 316\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, y_in, memory, src_key_padding_mask)\u001B[0m\n\u001B[1;32m    314\u001B[0m tgt_key_padding_mask \u001B[38;5;241m=\u001B[39m (y_in \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_idx)\n\u001B[1;32m    315\u001B[0m tgt_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_subsequent_mask(Lt, y_in\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 316\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:465\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    462\u001B[0m tgt_is_causal \u001B[38;5;241m=\u001B[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 465\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    473\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(output)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:855\u001B[0m, in \u001B[0;36mTransformerDecoderLayer.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    853\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x))\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 855\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sa_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    856\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001B[1;32m    857\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:864\u001B[0m, in \u001B[0;36mTransformerDecoderLayer._sa_block\u001B[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_sa_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor,\n\u001B[1;32m    863\u001B[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 864\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/activation.py:1196\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   1193\u001B[0m         merged_mask, mask_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerge_masks(attn_mask, key_padding_mask, query)\n\u001B[1;32m   1195\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1196\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_native_multi_head_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m                \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m                \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m                \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmerged_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[43m                \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m                \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmask_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1211\u001B[0m any_nested \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mis_nested \u001B[38;5;129;01mor\u001B[39;00m key\u001B[38;5;241m.\u001B[39mis_nested \u001B[38;5;129;01mor\u001B[39;00m value\u001B[38;5;241m.\u001B[39mis_nested\n\u001B[1;32m   1212\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m any_nested, (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMultiheadAttention does not support NestedTensor outside of its fast path. \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m   1213\u001B[0m                         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe fast path was not hit because \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwhy_not_fast_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "    # Agentic seq2seq — Routing with Dynamic→Static (CPU-only, no autotune)\n",
    "\n",
    "    from __future__ import annotations\n",
    "    from dataclasses import dataclass\n",
    "    from typing import List, Optional, Sequence, Tuple, Dict\n",
    "    import os, random, tempfile\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    import re\n",
    "    from typing import List, Optional, Sequence, Tuple, Dict, Callable\n",
    "\n",
    "    # ============================================================\n",
    "    # Repro (CPU-only)\n",
    "    # ============================================================\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "    # ===== Fixed role indices for strict pipeline =====\n",
    "    AGENT_ISSUE_ANALYSIS   = 0     # was AGENT_ISSUE_ANALYSIS\n",
    "    AGENT_CODE_GENERATION  = 1\n",
    "\n",
    "    def agent_pretty_name(agent_id: int) -> str:\n",
    "        return \"Issue Analysis Agent\" if agent_id == AGENT_ISSUE_ANALYSIS else (\n",
    "               \"Code Generation Agent\" if agent_id == AGENT_CODE_GENERATION else f\"Agent {agent_id}\"\n",
    "        )\n",
    "\n",
    "    def set_seed(s: int = 42):\n",
    "        random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "\n",
    "    # ============================================================\n",
    "    # Config\n",
    "    # ============================================================\n",
    "    @dataclass\n",
    "    class Config:\n",
    "        seed: int = 42\n",
    "        # data\n",
    "        limit: int = 1024\n",
    "        max_in_len: int = 1024\n",
    "        max_out_len: int = 256\n",
    "        spm_vocab: int = 8000\n",
    "        demo_data: bool = False         # False = load SWE-bench via HF datasets\n",
    "        # model\n",
    "        n_agents: int = 2\n",
    "        model_dim: int = 384\n",
    "        n_heads: int = 6\n",
    "        n_layers_enc: int = 4\n",
    "        n_layers_dec: int = 4\n",
    "        max_len_cap: int = 1024\n",
    "        # pipeline training (global)\n",
    "        pipe_epochs: int = 4\n",
    "        pipe_batch: int = 8\n",
    "        pipe_lr: float = 2e-4\n",
    "        lb_lambda: float = 5\n",
    "        router_lambda: float = 1.0\n",
    "        # static fine-tuning (specialization)\n",
    "        ft_epochs: int = 8\n",
    "        ft_batch: int = 8\n",
    "        ft_lr: float = 1e-4\n",
    "        agent_idx: int = 0\n",
    "        ft_unfreeze_adapters: bool = True\n",
    "        ft_unfreeze_dec_norms: bool = True\n",
    "        # decode / dump\n",
    "        decode_max_len: int = 160\n",
    "        out_dir: str = \"preds_static_role\"\n",
    "\n",
    "    CFG = Config()\n",
    "\n",
    "    # ============================================================\n",
    "    # Tokenizer: SentencePiece UNIGRAM (required)\n",
    "    # ============================================================\n",
    "    SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "    UNK, PAD, BOS, EOS = range(4)\n",
    "\n",
    "    try:\n",
    "        import sentencepiece as spm\n",
    "        HAVE_SPM = True\n",
    "    except ImportError:\n",
    "        HAVE_SPM = False\n",
    "\n",
    "    class SubwordTokenizer:\n",
    "        \"\"\"SPM UNIGRAM tokenizer trained on provided texts. No whitespace fallback.\"\"\"\n",
    "        def __init__(self, texts: Sequence[str], vocab_size: int = 8000, quiet: bool = True):\n",
    "            if not HAVE_SPM:\n",
    "                raise RuntimeError(\"SentencePiece missing. Install with: pip install sentencepiece\")\n",
    "            if vocab_size < 128:\n",
    "                raise ValueError(\"spm_vocab must be >= 128\")\n",
    "\n",
    "            import contextlib\n",
    "\n",
    "            @contextlib.contextmanager\n",
    "            def _silence_cpp_stdio():\n",
    "                try:\n",
    "                    import sys\n",
    "                    sys.stdout.flush(); sys.stderr.flush()\n",
    "                    devnull_fd = os.open(os.devnull, os.O_WRONLY)\n",
    "                    saved_out, saved_err = os.dup(1), os.dup(2)\n",
    "                    try:\n",
    "                        os.dup2(devnull_fd, 1); os.dup2(devnull_fd, 2)\n",
    "                        yield\n",
    "                    finally:\n",
    "                        os.dup2(saved_out, 1); os.dup2(saved_err, 2)\n",
    "                        os.close(saved_out); os.close(saved_err); os.close(devnull_fd)\n",
    "                except Exception:\n",
    "                    yield\n",
    "\n",
    "            self.quiet = quiet\n",
    "            with tempfile.TemporaryDirectory() as tmpd:\n",
    "                corpus = os.path.join(tmpd, \"spm_corpus.txt\")\n",
    "                with open(corpus, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for t in texts:\n",
    "                        f.write(str(t).replace(\"\\r\", \" \") + \"\\n\")\n",
    "\n",
    "                model_prefix = os.path.join(tmpd, \"spm_model\")\n",
    "                target_vocab = int(min(vocab_size, 80000))\n",
    "\n",
    "                cmd = (\n",
    "                    f\"--input={corpus} \"\n",
    "                    f\"--model_prefix={model_prefix} \"\n",
    "                    f\"--vocab_size={target_vocab} \"\n",
    "                    f\"--character_coverage=0.9995 \"\n",
    "                    f\"--model_type=unigram \"\n",
    "                    f\"--pad_id=1 --unk_id=0 --bos_id=2 --eos_id=3 \"\n",
    "                    f\"--hard_vocab_limit=false \"\n",
    "                    f\"--byte_fallback=true \"\n",
    "                    f\"--split_by_whitespace=false \"\n",
    "                    f\"--input_sentence_size=0 \"\n",
    "                    f\"--max_sentence_length=20000\"\n",
    "                )\n",
    "                with _silence_cpp_stdio():\n",
    "                    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "                self.sp = spm.SentencePieceProcessor()\n",
    "                self.sp.load(f\"{model_prefix}.model\")\n",
    "\n",
    "            self.vocab_size = self.sp.get_piece_size()\n",
    "            self.pad_idx, self.unk_idx, self.bos_idx, self.eos_idx = 1, 0, 2, 3\n",
    "\n",
    "        def encode(self, text: str, add_bos_eos: bool, max_len: int) -> torch.Tensor:\n",
    "            ids = self.sp.encode(str(text), out_type=int)\n",
    "            if add_bos_eos:\n",
    "                ids = [self.bos_idx] + ids + [self.eos_idx]\n",
    "            ids = ids[:max_len] or [self.unk_idx]\n",
    "            return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        def decode(self, ids: List[int]) -> str:\n",
    "            return self.sp.decode(ids)\n",
    "\n",
    "        @property\n",
    "        def pad(self): return self.pad_idx\n",
    "        @property\n",
    "        def bos(self): return self.bos_idx\n",
    "        @property\n",
    "        def eos(self): return self.eos_idx\n",
    "\n",
    "    def _extract_tag_block(text: str, tag: str) -> str:\n",
    "        open_tag, close_tag = f\"<{tag}>\", f\"</{tag}>\"\n",
    "        if open_tag in text and close_tag in text:\n",
    "            return text.split(open_tag, 1)[1].split(close_tag, 1)[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # Data loading / batching\n",
    "    # ============================================================\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        HAVE_HF = True\n",
    "    except Exception:\n",
    "        HAVE_HF = False\n",
    "\n",
    "    class SWEText2PatchData:\n",
    "        def __init__(self, *, split: str = \"train\", limit: Optional[int] = 1024,\n",
    "                     max_in_len: int = 512, max_out_len: int = 256,\n",
    "                     spm_vocab_size: int = 8000, demo_data: bool = True):\n",
    "            if demo_data:\n",
    "                print(\"[Data] DEMO synthetic dataset\")\n",
    "                rng = random.Random()\n",
    "                self.samples: List[Tuple[str, str, str]] = []\n",
    "                n = int(limit or 1024)\n",
    "                for i in range(n):\n",
    "                    title = f\"Issue {i}: Widget broken\"\n",
    "                    body = f\"Repro {i}: click→crash, trace={rng.randint(0,999)}\"\n",
    "                    patch = f\"diff --git a/app.py b/app.py\\n+print('fix {i}')\\n\"\n",
    "                    self.samples.append((f\"demo-{i}\", title + \"\\n\" + body, patch))\n",
    "                rng.shuffle(self.samples)\n",
    "\n",
    "                texts = [x for _, x, _ in self.samples] + [y for _, _, y in self.samples]\n",
    "                special_tag_text = \" \".join([\n",
    "                    \"<ISSUE_TITLE>\", \"</ISSUE_TITLE>\",\n",
    "                    \"<ISSUE_DESC>\",  \"</ISSUE_DESC>\",\n",
    "                    \"<HINTS>\",       \"</HINTS>\",\n",
    "                    \"<ISSUE_GIST>\",  \"</ISSUE_GIST>\",   # NEW tag used in pipeline context\n",
    "                ])\n",
    "                texts = texts + [special_tag_text] * 100\n",
    "                self.tok = SubwordTokenizer(texts, vocab_size=spm_vocab_size)\n",
    "                self.max_in_len, self.max_out_len = max_in_len, max_out_len\n",
    "                return\n",
    "\n",
    "            if not HAVE_HF:\n",
    "                raise RuntimeError(\"Install `datasets` to use SWE-bench: pip install datasets\")\n",
    "\n",
    "            print(\"[Data] Load SWE-bench…\")\n",
    "            ds = load_dataset(\"princeton-nlp/SWE-bench\", split=split)\n",
    "            if limit is not None:\n",
    "                ds = ds.select(range(min(limit, len(ds))))\n",
    "            rows = list(ds)\n",
    "\n",
    "            def build_input(ex: Dict) -> str:\n",
    "                title = str(ex.get(\"title\", \"\")).strip()\n",
    "                desc  = str(ex.get(\"problem_statement\", \"\")).strip()\n",
    "                hints = str(ex.get(\"hints_text\", \"\")).strip()\n",
    "                tagged = []\n",
    "                if title: tagged.append(f\"<ISSUE_TITLE>\\n{title}\\n</ISSUE_TITLE>\")\n",
    "                if desc:  tagged.append(f\"<ISSUE_DESC>\\n{desc}\\n</ISSUE_DESC>\")\n",
    "                if hints: tagged.append(f\"<HINTS>\\n{hints}\\n</HINTS>\")\n",
    "                meta = []\n",
    "                if ex.get(\"repo\"): meta.append(f\"repo={ex['repo']}\")\n",
    "                if ex.get(\"base_commit\"): meta.append(f\"base={ex['base_commit']}\")\n",
    "                if meta: tagged.append(\"[\" + \", \".join(meta) + \"]\")\n",
    "                return \"\\n\".join(tagged)\n",
    "\n",
    "            def pick_patch(ex: Dict) -> str:\n",
    "                for key in (\"patch\", \"base_patch\", \"model_patch\", \"test_patch\"):\n",
    "                    if key in ex and ex[key]: return str(ex[key])\n",
    "                return \"\"\n",
    "\n",
    "            self.samples: List[Tuple[str, str, str]] = []\n",
    "            for ex in rows:\n",
    "                iid = str(ex.get(\"instance_id\", \"\"))\n",
    "                xin = build_input(ex); yout = pick_patch(ex)\n",
    "                if len(yout.strip()) == 0: continue\n",
    "                self.samples.append((iid, xin, yout))\n",
    "\n",
    "            print(f\"[Data] {len(self.samples)} supervised pairs\")\n",
    "            texts = [x for _, x, _ in self.samples] + [y for _, _, y in self.samples]\n",
    "            special_tag_text = \" \".join([\n",
    "                \"<ISSUE_TITLE>\", \"</ISSUE_TITLE>\",\n",
    "                \"<ISSUE_DESC>\",  \"</ISSUE_DESC>\",\n",
    "                \"<HINTS>\",       \"</HINTS>\",\n",
    "                \"<ISSUE_GIST>\",        \"</ISSUE_GIST>\",\n",
    "            ])\n",
    "            texts = texts + [special_tag_text] * 100\n",
    "            self.tok = SubwordTokenizer(texts, vocab_size=spm_vocab_size)\n",
    "            self.max_in_len, self.max_out_len = max_in_len, max_out_len\n",
    "\n",
    "        def as_tensors(self) -> Tuple[List[str], torch.Tensor, torch.Tensor]:\n",
    "            if not getattr(self, \"samples\", None):\n",
    "                raise ValueError(\"No samples loaded.\")\n",
    "            ids: List[str] = []\n",
    "            xs: List[torch.Tensor] = []\n",
    "            ys: List[torch.Tensor] = []\n",
    "            for iid, x, y in self.samples:\n",
    "                ids.append(iid)\n",
    "                xs.append(self.tok.encode(x, add_bos_eos=False, max_len=self.max_in_len))\n",
    "                ys.append(self.tok.encode(y, add_bos_eos=True,  max_len=self.max_out_len))\n",
    "            X = pad_sequence(xs, batch_first=True, padding_value=self.tok.pad)\n",
    "            Y = pad_sequence(ys, batch_first=True, padding_value=self.tok.pad)\n",
    "            return ids, X, Y\n",
    "\n",
    "        def as_tensors_with_issue_targets(self, issue_max_len: int) -> Tuple[List[str], torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            ids, X, Y = self.as_tensors()\n",
    "            Ps = []\n",
    "            for _, xin, _ in self.samples:\n",
    "                title = _extract_tag_block(xin, \"ISSUE_TITLE\")\n",
    "                desc  = _extract_tag_block(xin, \"ISSUE_DESC\") or xin\n",
    "                issue  = _clean_issue_text(_make_issue_gist(title, desc))\n",
    "                if not issue:\n",
    "                    issue = _clean_issue_text(desc)\n",
    "                Ps.append(self.tok.encode(issue, add_bos_eos=True, max_len=issue_max_len))\n",
    "            P = pad_sequence(Ps, batch_first=True, padding_value=self.tok.pad)\n",
    "            return ids, X, Y, P\n",
    "\n",
    "    # ============================================================\n",
    "    # Core model building blocks\n",
    "    # ============================================================\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, vocab_size: int, model_dim: int = 512, n_heads: int = 8,\n",
    "                     n_layers: int = 6, max_len: int = 1024, pad_token_id: int = 0):\n",
    "            super().__init__()\n",
    "            self.pad_token_id = pad_token_id\n",
    "            self.tok_embedding = nn.Embedding(vocab_size, model_dim, padding_idx=pad_token_id)\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(1, max_len, model_dim) * 0.01)\n",
    "            layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "            self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            B, T = x.shape\n",
    "            h = self.tok_embedding(x) + self.pos_embedding[:, :T, :]\n",
    "            mask = (x == self.pad_token_id)\n",
    "            mem = self.encoder(h, src_key_padding_mask=mask)\n",
    "            valid = (~mask).float()\n",
    "            denom = valid.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "            pooled = (mem * valid.unsqueeze(-1)).sum(dim=1) / denom\n",
    "            return mem, pooled, mask\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, vocab_size: int, model_dim: int = 512, n_heads: int = 8,\n",
    "                     n_layers: int = 6, max_len: int = 1024, pad_idx: int = PAD,\n",
    "                     tok_embedding: Optional[nn.Embedding] = None):\n",
    "            super().__init__()\n",
    "            self.pad_idx = pad_idx\n",
    "            self.tok_embedding = tok_embedding if tok_embedding is not None else nn.Embedding(vocab_size, model_dim, padding_idx=pad_idx)\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(1, max_len, model_dim) * 0.01)\n",
    "            layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True)\n",
    "            self.decoder = nn.TransformerDecoder(layer, num_layers=n_layers)\n",
    "\n",
    "        def _subsequent_mask(self, L: int, device) -> torch.Tensor:\n",
    "            return torch.triu(torch.ones(L, L, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "        def forward(self, y_in: torch.Tensor, memory: torch.Tensor, src_key_padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "            B, Lt = y_in.shape\n",
    "            y_emb = self.tok_embedding(y_in) + self.pos_embedding[:, :Lt, :]\n",
    "            tgt_key_padding_mask = (y_in == self.pad_idx)\n",
    "            tgt_mask = self._subsequent_mask(Lt, y_in.device)\n",
    "            return self.decoder(y_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                memory_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "    class Agent(nn.Module):\n",
    "        def __init__(self, model_dim: int, vocab_size: int, adapter_dim: int = 124):\n",
    "            super().__init__()\n",
    "            self.adapter = nn.Sequential(\n",
    "                nn.LayerNorm(model_dim),\n",
    "                nn.Linear(model_dim, adapter_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(adapter_dim, model_dim),\n",
    "            )\n",
    "            self.router_head = nn.Linear(model_dim, vocab_size)  # kept to preserve shape, unused\n",
    "            self.role_head   = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "        def project(self, states: torch.Tensor, head: str = \"role\") -> torch.Tensor:\n",
    "            h = self.adapter(states)\n",
    "            layer = self.router_head if head == \"router\" else self.role_head\n",
    "            return layer(h)\n",
    "\n",
    "    class StrictPipeline(nn.Module):\n",
    "        \"\"\"\n",
    "        Strict A→B pipeline on static role heads:\n",
    "          issue = Agent A(analysis) generates from full X\n",
    "          gist_only = <ISSUE_GIST>...</ISSUE_GIST> produced from A\n",
    "          patch = Agent B(code) generates from gist_only   # (no original issue text)\n",
    "        \"\"\"\n",
    "        def __init__(self, agents: nn.ModuleList):\n",
    "            super().__init__()\n",
    "            self.agents = agents\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def run(self, model: \"AgenticTransformerSeq2Seq\", tok: \"SubwordTokenizer\", X: torch.Tensor,\n",
    "                *, issue_max_len: int, out_max_len: int, max_in_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            issue_ctx, issue_display_ids = _issue_ctx_greedy_with_fallback(\n",
    "                model, tok, X, issue_max_len=issue_max_len\n",
    "            )\n",
    "            gist_only = issue_ctx[:, :max_in_len]  # ← CODE agent sees only the gist\n",
    "            patch_ids = _generate_static(\n",
    "                model, gist_only, agent_id=AGENT_CODE_GENERATION, max_len=out_max_len,\n",
    "                top_k=50, top_p=0.95, temperature=0.9, no_repeat_ngram_size=3, min_len=24\n",
    "            )\n",
    "\n",
    "            return issue_display_ids, patch_ids\n",
    "\n",
    "    class AssignmentModule:\n",
    "        def __init__(self, n_agents: int): self.n_agents = n_agents\n",
    "        def __call__(self, user_id: int) -> int:\n",
    "            if isinstance(user_id, torch.Tensor): return int((user_id % self.n_agents).item())\n",
    "            return int(user_id) % self.n_agents\n",
    "\n",
    "    class RoutingModule(nn.Module):\n",
    "        \"\"\"Static routing via AssignmentModule + Strict A→B pipeline.\"\"\"\n",
    "        def __init__(self, agents: nn.ModuleList):\n",
    "            super().__init__()\n",
    "            self.agents = agents\n",
    "            self.assign = AssignmentModule(n_agents=len(agents))\n",
    "            self.pipeline = StrictPipeline(agents)\n",
    "\n",
    "        def project_role(self, dec_states: torch.Tensor, *, agent_id: int) -> torch.Tensor:\n",
    "            return self.agents[agent_id].project(dec_states, head=\"role\")\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def run_pipeline(self, model: \"AgenticTransformerSeq2Seq\", tok: \"SubwordTokenizer\", X: torch.Tensor,\n",
    "                         *, issue_max_len: int, out_max_len: int, max_in_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            return self.pipeline.run(model, tok, X, issue_max_len=issue_max_len, out_max_len=out_max_len, max_in_len=max_in_len)\n",
    "\n",
    "    class AgenticTransformerSeq2Seq(nn.Module):\n",
    "        def __init__(self, vocab_size: int, n_agents: int = 2, model_dim: int = 512,\n",
    "                     n_heads: int = 8, n_layers_enc: int = 6, n_layers_dec: int = 6,\n",
    "                     max_len: int = 1024, pad_idx: int = PAD):\n",
    "            super().__init__()\n",
    "            self.encoder = Encoder(vocab_size, model_dim, n_heads, n_layers_enc, max_len, pad_idx)\n",
    "            self.decoder = Decoder(vocab_size, model_dim, n_heads, n_layers_dec, max_len, pad_idx,\n",
    "                                   tok_embedding=self.encoder.tok_embedding)\n",
    "            agents = nn.ModuleList([Agent(model_dim, vocab_size) for _ in range(n_agents)])\n",
    "            self.routing = RoutingModule(agents)\n",
    "            self.pad_idx = pad_idx\n",
    "\n",
    "        def encode(self, x: torch.Tensor):\n",
    "            return self.encoder(x)\n",
    "\n",
    "        def decode_states(self, y_in: torch.Tensor, memory: torch.Tensor, src_key_padding_mask: torch.Tensor):\n",
    "            return self.decoder(y_in, memory, src_key_padding_mask)\n",
    "\n",
    "        def forward_role(self, x: torch.Tensor, y_in: torch.Tensor, *, agent_id: int):\n",
    "            mem, _cls, src_mask = self.encode(x)\n",
    "            dec_states = self.decode_states(y_in, mem, src_mask)\n",
    "            return self.routing.project_role(dec_states, agent_id=agent_id)\n",
    "\n",
    "    # ============================================================\n",
    "    # Decoding & generation (static inference path)\n",
    "    # ============================================================\n",
    "    @torch.no_grad()\n",
    "    def _generate_static(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        X: torch.Tensor,\n",
    "        *,\n",
    "        agent_id: int,\n",
    "        max_len: int,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        temperature: float = 1.0,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        min_len: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Greedy by default; with top_k/top_p uses constrained sampling. Static agent selection.\"\"\"\n",
    "        model.eval()\n",
    "        memory, _cls, src_mask = model.encode(X)\n",
    "        B = X.size(0)\n",
    "        vocab_size = model.encoder.tok_embedding.num_embeddings\n",
    "        ys = torch.full((B, 1), BOS, dtype=torch.long, device=X.device)\n",
    "\n",
    "        for _t in range(1, max_len):\n",
    "            dec = model.decode_states(ys, memory, src_mask)\n",
    "            step_logits = model.routing.agents[agent_id].project(dec[:, -1:], head=\"role\").squeeze(1)\n",
    "\n",
    "            # Block EOS until min_len is reached\n",
    "            if ys.size(1) < max(1, min_len):\n",
    "                step_logits[:, EOS] = float(\"-inf\")\n",
    "\n",
    "            # No-repeat n-gram mask\n",
    "            if no_repeat_ngram_size and no_repeat_ngram_size > 0:\n",
    "                banned = _no_repeat_ngram_mask(ys, no_repeat_ngram_size, vocab_size)\n",
    "                step_logits = step_logits.masked_fill(banned, float(\"-inf\"))\n",
    "\n",
    "            # Temperature\n",
    "            if temperature and temperature != 1.0:\n",
    "                step_logits = step_logits / max(temperature, 1e-8)\n",
    "\n",
    "            # Sampling vs greedy\n",
    "            use_sampling = (top_k is not None and top_k > 0) or (top_p is not None and 0.0 < top_p < 1.0)\n",
    "            if use_sampling:\n",
    "                logits = _top_k_top_p_filtering(step_logits.clone(), top_k, top_p)\n",
    "                next_tok = torch.distributions.Categorical(logits=logits).sample().unsqueeze(1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(step_logits, dim=-1, keepdim=True)\n",
    "\n",
    "            ys = torch.cat([ys, next_tok], dim=1)\n",
    "            if (next_tok == EOS).all():\n",
    "                break\n",
    "        return ys\n",
    "\n",
    "    # --- Sampling utilities ---\n",
    "    def _no_repeat_ngram_mask(ys: torch.Tensor, n: int, vocab_size: int) -> torch.Tensor:\n",
    "        if n <= 0: return torch.zeros((ys.size(0), vocab_size), dtype=torch.bool, device=ys.device)\n",
    "        B, L = ys.shape\n",
    "        mask = torch.zeros((B, vocab_size), dtype=torch.bool, device=ys.device)\n",
    "        if L < n: return mask\n",
    "        for b in range(B):\n",
    "            seq = ys[b].tolist()\n",
    "            prefix2next = {}\n",
    "            for i in range(L - n + 1):\n",
    "                prefix = tuple(seq[i:i + n - 1]); nxt = seq[i + n - 1]\n",
    "                prefix2next.setdefault(prefix, set()).add(nxt)\n",
    "            last_prefix = tuple(seq[-(n - 1):]) if n > 1 else tuple()\n",
    "            banned = prefix2next.get(last_prefix, set())\n",
    "            if banned: mask[b, list(banned)] = True\n",
    "        return mask\n",
    "\n",
    "    def _top_k_top_p_filtering(logits: torch.Tensor, top_k: Optional[int], top_p: Optional[float]) -> torch.Tensor:\n",
    "        if top_k is not None and top_k > 0:\n",
    "            k = min(top_k, logits.size(-1))\n",
    "            thresh = torch.topk(logits, k, dim=-1).values[..., -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < thresh, torch.full_like(logits, float(\"-inf\")), logits)\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
    "            cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "            to_mask = cum > top_p\n",
    "            to_mask[..., 1:] = to_mask[..., :-1].clone()\n",
    "            to_mask[..., 0] = False\n",
    "            logits.scatter_(1, sorted_idx, torch.where(to_mask, torch.full_like(sorted_probs, float(\"-inf\")), logits.gather(1, sorted_idx)))\n",
    "        return logits\n",
    "\n",
    "    # ============================================================\n",
    "    # Training utilities: losses, metrics, targets\n",
    "    # ============================================================\n",
    "    class SeqCELoss(nn.Module):\n",
    "        def __init__(self, pad_idx: int):\n",
    "            super().__init__()\n",
    "            self.ce = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "        def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "            B, L, V = logits.shape\n",
    "            return self.ce(logits.reshape(B*L, V), targets.reshape(B*L))\n",
    "\n",
    "    def shift_targets(y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return y[:, :-1], y[:, 1:]\n",
    "\n",
    "    # ============================================================\n",
    "    # Training loops (specialization & pipeline)\n",
    "    # ============================================================\n",
    "    def train_strict_pipeline_swebench(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        X_train: torch.Tensor,\n",
    "        Y_train: torch.Tensor,\n",
    "        *,\n",
    "        tok: \"SubwordTokenizer\",\n",
    "        issue_max_len: int = 124,\n",
    "        epochs: int = 3,\n",
    "        batch_size: int = 8,\n",
    "        lr: float = 2e-4,\n",
    "        device: str = DEVICE,\n",
    "        unfreeze_backbone: bool = True,\n",
    "        unfreeze_B_adapter: bool = True,\n",
    "        unfreeze_dec_norms: bool = True,\n",
    "        max_in_len: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Strict ISSUE_ANALYSIS → CODE_GENERATION training.\"\"\"\n",
    "        assert AGENT_ISSUE_ANALYSIS == 0 and AGENT_CODE_GENERATION == 1, \"Expect issue=0, code=1.\"\n",
    "        model.to(device)\n",
    "        print(\"[Agentic][Training][CODE] starting (teacher-forced with gist context)\", flush=True)\n",
    "\n",
    "        _set_trainable_strict_agent(\n",
    "            model,\n",
    "            agent_id=AGENT_CODE_GENERATION,\n",
    "            unfreeze_backbone=unfreeze_backbone,\n",
    "            unfreeze_adapter=unfreeze_B_adapter,\n",
    "            unfreeze_dec_norms=unfreeze_dec_norms,\n",
    "        )\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        opt = optim.Adam(params, lr=lr)\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "\n",
    "        N = X_train.size(0)\n",
    "        max_in_len = int(max_in_len or X_train.size(1))\n",
    "\n",
    "        for ep in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            epoch_loss_sum = 0.0\n",
    "            epoch_tok_correct = 0\n",
    "            epoch_tok_total = 0\n",
    "\n",
    "            for i in range(0, N, batch_size):\n",
    "                xb = X_train[i:i + batch_size].to(device)\n",
    "                yb = Y_train[i:i + batch_size].to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    issue_ctx, _ = _issue_ctx_greedy_with_fallback(\n",
    "                        model, tok, xb, issue_max_len=issue_max_len\n",
    "                    )\n",
    "                    xb_gist = issue_ctx[:, :max_in_len]  # ← with gist input\n",
    "\n",
    "                y_in, y_tgt = shift_targets(yb)\n",
    "                logits = model.forward_role(xb_gist, y_in, agent_id=AGENT_CODE_GENERATION)\n",
    "                loss = loss_fn(logits, y_tgt)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(params, 1.0)\n",
    "                opt.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    Bsz, _, _ = logits.shape\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    mask = (y_tgt != model.pad_idx)\n",
    "                    epoch_loss_sum += float(loss.detach()) * Bsz\n",
    "                    epoch_tok_correct += ((preds == y_tgt) & mask).sum().item()\n",
    "                    epoch_tok_total += mask.sum().item()\n",
    "\n",
    "            epoch_ce = epoch_loss_sum / float(N)\n",
    "            epoch_acc = (epoch_tok_correct / max(epoch_tok_total, 1)) if epoch_tok_total > 0 else 0.0\n",
    "            print(f\"[Agentic][Training][CODE][Epoch {ep}] CE={epoch_ce:.3f} | tok_acc={epoch_acc:.3f}\")\n",
    "        print(\"[Agentic][Training][CODE] done ✅\", flush=True)\n",
    "\n",
    "    def train_issue_supervised(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        X_train: torch.Tensor,\n",
    "        P_train: torch.Tensor,\n",
    "        *,\n",
    "        epochs: int = 2,\n",
    "        batch_size: int = 8,\n",
    "        lr: float = 2e-4,\n",
    "        device: str = DEVICE,\n",
    "        unfreeze_backbone: bool = True,\n",
    "        unfreeze_A_adapter: bool = True,\n",
    "        unfreeze_dec_norms: bool = True,\n",
    "    ):\n",
    "        \"\"\"Teacher-force Agent 0 (ISSUE_ANALYSIS) to generate ISSUE_DESC.\"\"\"\n",
    "        model.to(device)\n",
    "        print(\"[Agentic][Training][ISSUE] starting\", flush=True)\n",
    "\n",
    "        _set_trainable_strict_agent(\n",
    "            model,\n",
    "            agent_id=AGENT_ISSUE_ANALYSIS,\n",
    "            unfreeze_backbone=unfreeze_backbone,\n",
    "            unfreeze_adapter=unfreeze_A_adapter,\n",
    "            unfreeze_dec_norms=unfreeze_dec_norms,\n",
    "        )\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        opt = optim.Adam(params, lr=lr)\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "\n",
    "        N = X_train.size(0)\n",
    "        for ep in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            sum_loss, tok_correct, tok_total = 0.0, 0, 0\n",
    "            for i in range(0, N, batch_size):\n",
    "                xb = X_train[i:i+batch_size].to(device)\n",
    "                pb = P_train[i:i+batch_size].to(device)\n",
    "                y_in, y_tgt = shift_targets(pb)\n",
    "                logits = model.forward_role(xb, y_in, agent_id=AGENT_ISSUE_ANALYSIS)\n",
    "                loss = loss_fn(logits, y_tgt)\n",
    "\n",
    "                opt.zero_grad(); loss.backward()\n",
    "                nn.utils.clip_grad_norm_(params, 1.0)\n",
    "                opt.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    mask  = (y_tgt != model.pad_idx)\n",
    "                    tok_correct += ((preds == y_tgt) & mask).sum().item()\n",
    "                    tok_total   += mask.sum().item()\n",
    "                    sum_loss    += float(loss.detach()) * xb.size(0)\n",
    "\n",
    "            print(f\"[Agentic][Training][ISSUE][Epoch {ep}] CE={sum_loss/float(N):.3f} | tok_acc={(tok_correct/max(tok_total,1)):.3f}\")\n",
    "        print(\"[Agentic][Training][ISSUE] done ✅\", flush=True)\n",
    "\n",
    "    def fine_tune_static(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        X: torch.Tensor,\n",
    "        Y: torch.Tensor,\n",
    "        *,\n",
    "        user_id: int,\n",
    "        epochs: int = 3,\n",
    "        batch_size: int = 8,\n",
    "        lr: float = 1e-4,                 # lower LR\n",
    "        weight_decay: float = 0.01,       # add wd\n",
    "        unfreeze_adapters: bool = True,\n",
    "        unfreeze_dec_norms: bool = True,\n",
    "        unfreeze_decoder_tail_blocks: int = 1,   # tiny extra capacity if desired\n",
    "        idxs: Optional[torch.Tensor] = None,\n",
    "        device: str = DEVICE,\n",
    "        tok: Optional[\"SubwordTokenizer\"] = None,\n",
    "        P: Optional[torch.Tensor] = None,         # gold ISSUE_DESC (targets for Issue agent)\n",
    "        gist_ctx_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,  # for Code agent\n",
    "        max_in_len: Optional[int] = None,\n",
    "        use_concat_first_epoch: bool = True,      # (Code only) concat gist + original X for epoch 1\n",
    "        patience: int = 2                         # early stopping on Dev CE\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stage-2 static specialization for the selected agent.\n",
    "          - If user_id == AGENT_CODE_GENERATION (1): curriculum with gist context (unchanged).\n",
    "          - If user_id == AGENT_ISSUE_ANALYSIS   (0): train on original X, targets=P (gold ISSUE_DESC).\n",
    "        Backbone stays frozen except for explicitly allowed parts (adapters/dec norms/tail blocks).\n",
    "        \"\"\"\n",
    "        # Sanity: for Issue agent we need P (targets), for Code agent we need Y (patch targets)\n",
    "        if user_id == AGENT_ISSUE_ANALYSIS and P is None:\n",
    "            raise ValueError(\"fine_tune_static(issue): P (gold ISSUE_DESC) is required.\")\n",
    "        if user_id == AGENT_CODE_GENERATION and gist_ctx_fn is None:\n",
    "            raise ValueError(\"fine_tune_static(code): gist_ctx_fn is required for gist curriculum.\")\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Freeze everything; unfreeze only this agent (+optional norms/decoder tail)\n",
    "        _set_ft_requires_grad(\n",
    "            model,\n",
    "            user_id=user_id,\n",
    "            unfreeze_adapters=unfreeze_adapters,\n",
    "            unfreeze_dec_norms=unfreeze_dec_norms\n",
    "        )\n",
    "        if unfreeze_decoder_tail_blocks and unfreeze_decoder_tail_blocks > 0:\n",
    "            _unfreeze_decoder_tail(model, n_last_blocks=int(unfreeze_decoder_tail_blocks))\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        opt = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "\n",
    "        # Slice to optional subset\n",
    "        xb_all = X if idxs is None else X[idxs]\n",
    "        # Choose targets by agent\n",
    "        if user_id == AGENT_CODE_GENERATION:\n",
    "            tgt_all = Y if idxs is None else Y[idxs]\n",
    "        else:  # Issue agent\n",
    "            tgt_all = P if idxs is None else P[idxs]\n",
    "\n",
    "        N = xb_all.size(0)\n",
    "        max_in_len = int(max_in_len or xb_all.size(1))\n",
    "\n",
    "        # 90/10 tail split for dev\n",
    "        dev_frac = max(1, int(0.1 * N))\n",
    "        xb_tr, xb_dev = xb_all[:-dev_frac], xb_all[-dev_frac:]\n",
    "        tb_tr, tb_dev = tgt_all[:-dev_frac], tgt_all[-dev_frac:]\n",
    "        P_tr = P[:-dev_frac] if (P is not None) else None\n",
    "        P_dev = P[-dev_frac:] if (P is not None) else None\n",
    "\n",
    "        best_dev_ce = float(\"inf\")\n",
    "        bad_epochs = 0\n",
    "\n",
    "        for ep in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            ep_loss = 0.0\n",
    "            correct_train, total_train = 0, 0\n",
    "\n",
    "            # ===== Build contexts per agent =====\n",
    "            if user_id == AGENT_CODE_GENERATION:\n",
    "                # ---- Code agent: same gist curriculum as before ----\n",
    "                if ep == 1 and P is not None:\n",
    "                    with torch.no_grad():\n",
    "                        def _wrap_from_P(P_block):\n",
    "                            disp_rows = []\n",
    "                            for i in range(P_block.size(0)):\n",
    "                                ids = [t for t in P_block[i].tolist() if t not in (tok.pad, tok.bos, tok.eos)]\n",
    "                                disp_rows.append(tok.decode(ids))\n",
    "                            ctx_rows = []\n",
    "                            for g in disp_rows:\n",
    "                                ctx_txt = f\"<ISSUE_GIST>\\n{_postprocess_gist(g)}\\n</ISSUE_GIST>\"\n",
    "                                ctx_rows.append(torch.tensor(tok.sp.encode(ctx_txt, out_type=int), dtype=torch.long))\n",
    "                            return pad_sequence(ctx_rows, batch_first=True, padding_value=tok.pad)\n",
    "                    X_gist_clean_tr  = _wrap_from_P(P_tr)\n",
    "                    X_gist_clean_dev = _wrap_from_P(P_dev)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        X_gist_clean_tr  = gist_ctx_fn(xb_tr.to(device)).cpu()\n",
    "                        X_gist_clean_dev = gist_ctx_fn(xb_dev.to(device)).cpu()\n",
    "\n",
    "                if ep == 1 and use_concat_first_epoch:\n",
    "                    X_ctx_tr  = _concat_truncate(X_gist_clean_tr.to(device),  xb_tr.to(device),  max_len=max_in_len)\n",
    "                    X_ctx_dev = _concat_truncate(X_gist_clean_dev.to(device), xb_dev.to(device), max_len=max_in_len)\n",
    "                else:\n",
    "                    X_ctx_tr  = X_gist_clean_tr.to(device)[:,  :max_in_len]\n",
    "                    X_ctx_dev = X_gist_clean_dev.to(device)[:, :max_in_len]\n",
    "\n",
    "            else:\n",
    "                # ---- Issue agent: plain original X, no gist/concat curriculum ----\n",
    "                X_ctx_tr  = xb_tr.to(device)[:, :max_in_len]\n",
    "                X_ctx_dev = xb_dev.to(device)[:, :max_in_len]\n",
    "\n",
    "            # ===== TRAIN =====\n",
    "            for i in range(0, xb_tr.size(0), batch_size):\n",
    "                xb = X_ctx_tr[i:i+batch_size].to(device)\n",
    "                yb = tb_tr[i:i+batch_size].to(device)  # targets depend on agent (Y for code, P for issue)\n",
    "                y_in, y_tgt = shift_targets(yb)\n",
    "                logits = model.forward_role(xb, y_in, agent_id=user_id)\n",
    "                loss = loss_fn(logits, y_tgt)\n",
    "\n",
    "                preds = logits.argmax(-1)\n",
    "                correct_train += (preds == y_tgt).masked_select(y_tgt != model.pad_idx).sum().item()\n",
    "                total_train   += (y_tgt != model.pad_idx).sum().item()\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(params, 1.0)\n",
    "                opt.step()\n",
    "\n",
    "                ep_loss += float(loss.detach()) * xb.size(0)\n",
    "\n",
    "            train_acc = correct_train / max(total_train, 1)\n",
    "            train_ce  = ep_loss / float(max(len(xb_tr), 1))\n",
    "\n",
    "            # ===== DEV =====\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_in_dev, y_tgt_dev = shift_targets(tb_dev.to(device))\n",
    "                logits_dev = model.forward_role(X_ctx_dev.to(device), y_in_dev, agent_id=user_id)\n",
    "                dev_ce = float(loss_fn(logits_dev, y_tgt_dev).item())\n",
    "\n",
    "                preds_dev = logits_dev.argmax(-1)\n",
    "                correct_dev = (preds_dev == y_tgt_dev).masked_select(y_tgt_dev != model.pad_idx).sum().item()\n",
    "                total_dev   = (y_tgt_dev != model.pad_idx).sum().item()\n",
    "                dev_acc = correct_dev / max(total_dev, 1)\n",
    "\n",
    "            print(f\"[Agentic][Static Routing][{agent_pretty_name(user_id)} FT] \"\n",
    "                  f\"Epoch {ep} | TrainCE={train_ce:.3f} | TrainAcc={train_acc:.3f} \"\n",
    "                  f\"| DevCE={dev_ce:.3f} | DevAcc={dev_acc:.3f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if dev_ce + 1e-4 < best_dev_ce:\n",
    "                best_dev_ce = dev_ce\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= patience:\n",
    "                    print(\"[Agentic][Static Routing] Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "    # ============================================================\n",
    "    # Training helpers (freezing)\n",
    "    # ============================================================\n",
    "    def _set_ft_requires_grad(model: AgenticTransformerSeq2Seq, *, user_id: int, unfreeze_adapters: bool, unfreeze_dec_norms: bool):\n",
    "        for p in model.parameters(): p.requires_grad = False\n",
    "        if unfreeze_dec_norms:\n",
    "            for name, p in model.decoder.named_parameters():\n",
    "                if \"norm\" in name: p.requires_grad = True\n",
    "        idx = user_id % len(model.routing.agents)\n",
    "        ag = model.routing.agents[idx]\n",
    "        for name, p in ag.named_parameters():\n",
    "            if name.startswith(\"role_head\"): p.requires_grad = True\n",
    "            elif unfreeze_adapters and name.startswith(\"adapter\"): p.requires_grad = True\n",
    "\n",
    "    def _set_trainable_strict_agent(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        *,\n",
    "        agent_id: int = AGENT_CODE_GENERATION,\n",
    "        unfreeze_backbone: bool = True,\n",
    "        unfreeze_adapter: bool = True,\n",
    "        unfreeze_dec_norms: bool = True\n",
    "    ):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        ag = model.routing.agents[agent_id]\n",
    "        for name, p in ag.named_parameters():\n",
    "            if name.startswith(\"role_head\"): p.requires_grad = True\n",
    "            elif unfreeze_adapter and name.startswith(\"adapter\"): p.requires_grad = True\n",
    "        if unfreeze_backbone:\n",
    "            for p in model.encoder.parameters(): p.requires_grad = True\n",
    "            for p in model.decoder.parameters(): p.requires_grad = True\n",
    "        elif unfreeze_dec_norms:\n",
    "            for name, p in model.decoder.named_parameters():\n",
    "                if \"norm\" in name: p.requires_grad = True\n",
    "\n",
    "    def _wrap_issue_ids_with_tags(tok: \"SubwordTokenizer\", issue_ids: torch.Tensor) -> torch.Tensor:\n",
    "        rows: List[torch.Tensor] = []\n",
    "        B = issue_ids.size(0)\n",
    "        for i in range(B):\n",
    "            ids = [t for t in issue_ids[i].tolist() if t not in (tok.pad, tok.bos, tok.eos)]\n",
    "            issue_text = _clean_issue_text(tok.decode(ids))\n",
    "            wrapped = f\"<ISSUE_GIST>\\n{issue_text}\\n</ISSUE_GIST>\"\n",
    "            row = torch.tensor(tok.sp.encode(wrapped, out_type=int), dtype=torch.long)\n",
    "            rows.append(row if len(row) > 0 else torch.tensor([tok.pad], dtype=torch.long))\n",
    "        return pad_sequence(rows, batch_first=True, padding_value=tok.pad)\n",
    "\n",
    "    def _set_trainable_stage1_joint(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        *,\n",
    "        unfreeze_backbone: bool = True,\n",
    "        unfreeze_adapters: bool = True,\n",
    "        unfreeze_dec_norms: bool = True,\n",
    "    ):\n",
    "        for p in model.parameters(): p.requires_grad = False\n",
    "        for agent_id in (AGENT_ISSUE_ANALYSIS, AGENT_CODE_GENERATION):\n",
    "            ag = model.routing.agents[agent_id]\n",
    "            for name, p in ag.named_parameters():\n",
    "                if name.startswith(\"role_head\"): p.requires_grad = True\n",
    "                elif unfreeze_adapters and name.startswith(\"adapter\"): p.requires_grad = True\n",
    "        if unfreeze_backbone:\n",
    "            for p in model.encoder.parameters(): p.requires_grad = True\n",
    "            for p in model.decoder.parameters(): p.requires_grad = True\n",
    "        elif unfreeze_dec_norms:\n",
    "            for name, p in model.decoder.named_parameters():\n",
    "                if \"norm\" in name: p.requires_grad = True\n",
    "\n",
    "    def train_stage1_interleaved(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        X_train: torch.Tensor,\n",
    "        Y_train: torch.Tensor,\n",
    "        P_train: torch.Tensor,\n",
    "        *,\n",
    "        tok: \"SubwordTokenizer\",\n",
    "        issue_max_len: int = 124,\n",
    "        epochs: int = 2,\n",
    "        batch_size: int = 8,\n",
    "        lr: float = 2e-4,\n",
    "        device: str = DEVICE,\n",
    "        unfreeze_backbone: bool = True,\n",
    "        unfreeze_adapters: bool = True,\n",
    "        unfreeze_dec_norms: bool = True,\n",
    "        max_in_len: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stage 1 (INTERLEAVED per epoch):\n",
    "          1) Train ISSUE (Agent 0) with teacher-forcing on ISSUE_DESC targets.\n",
    "          2) Generate <ISSUE> (no grad), append to X, train CODE (Agent 1) on patch targets.\n",
    "          Joint step uses SUM of both losses.\n",
    "        \"\"\"\n",
    "        assert AGENT_ISSUE_ANALYSIS == 0 and AGENT_CODE_GENERATION == 1\n",
    "        model.to(device)\n",
    "\n",
    "        _set_trainable_stage1_joint(model, unfreeze_backbone=unfreeze_backbone,\n",
    "                                    unfreeze_adapters=unfreeze_adapters,\n",
    "                                    unfreeze_dec_norms=unfreeze_dec_norms)\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        opt = optim.Adam(params, lr=lr)\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "\n",
    "        N = X_train.size(0)\n",
    "        max_in_len = int(max_in_len or X_train.size(1))\n",
    "\n",
    "        for ep in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            issue_loss_sum = code_loss_sum = 0.0\n",
    "            issue_tok_correct = issue_tok_total = 0\n",
    "            code_tok_correct = code_tok_total = 0\n",
    "\n",
    "            for i in range(0, N, batch_size):\n",
    "                xb = X_train[i:i+batch_size].to(device)\n",
    "                yb = Y_train[i:i+batch_size].to(device)\n",
    "                pb = P_train[i:i+batch_size].to(device)\n",
    "\n",
    "                # (1) ISSUE supervised\n",
    "                y_in_p, y_tgt_p = shift_targets(pb)\n",
    "                logits_p = model.forward_role(xb, y_in_p, agent_id=AGENT_ISSUE_ANALYSIS)\n",
    "                loss_p = loss_fn(logits_p, y_tgt_p)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_p = logits_p.argmax(dim=-1)\n",
    "                    mask_p  = (y_tgt_p != model.pad_idx)\n",
    "                    issue_tok_correct += ((preds_p == y_tgt_p) & mask_p).sum().item()\n",
    "                    issue_tok_total   += mask_p.sum().item()\n",
    "                    issue_loss_sum    += float(loss_p.detach()) * xb.size(0)\n",
    "\n",
    "                # Generate issue context (greedy + fallback)\n",
    "                with torch.no_grad():\n",
    "                    issue_ctx, _ = _issue_ctx_greedy_with_fallback(\n",
    "                        model, tok, xb, issue_max_len=issue_max_len\n",
    "                    )\n",
    "                    xb_gist = issue_ctx[:, :max_in_len]  # ← with gist input\n",
    "\n",
    "                # (2) CODE supervised on augmented input\n",
    "                y_in_c, y_tgt_c = shift_targets(yb)\n",
    "                logits_c = model.forward_role(xb_gist, y_in_c, agent_id=AGENT_CODE_GENERATION)\n",
    "                loss_c = loss_fn(logits_c, y_tgt_c)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_c = logits_c.argmax(dim=-1)\n",
    "                    mask_c  = (y_tgt_c != model.pad_idx)\n",
    "                    code_tok_correct += ((preds_c == y_tgt_c) & mask_c).sum().item()\n",
    "                    code_tok_total   += mask_c.sum().item()\n",
    "                    code_loss_sum    += float(loss_c.detach()) * xb.size(0)\n",
    "\n",
    "                # Joint step\n",
    "                loss = loss_p + loss_c\n",
    "                opt.zero_grad(); loss.backward()\n",
    "                nn.utils.clip_grad_norm_(params, 1.0)\n",
    "                opt.step()\n",
    "\n",
    "            issue_ce  = issue_loss_sum / float(N)\n",
    "            code_ce  = code_loss_sum / float(N)\n",
    "            issue_acc = (issue_tok_correct / max(issue_tok_total, 1)) if issue_tok_total > 0 else 0.0\n",
    "            code_acc = (code_tok_correct / max(code_tok_total, 1)) if code_tok_total > 0 else 0.0\n",
    "\n",
    "            print(\n",
    "                f\"[Agentic][Training][Epoch {ep}] \"\n",
    "                f\"ISSUE: CE={issue_ce:.3f} | tok_acc={issue_acc:.3f}  ||  \"\n",
    "                f\"CODE: CE={code_ce:.3f} | tok_acc={code_acc:.3f}  ||  \",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    def _postprocess_gist(txt: str) -> str:\n",
    "        # strong cleanup for display + context\n",
    "        txt = _clean_issue_text(txt)   # <-- was _clean_issue_text, which didn't exist\n",
    "        # remove backticks/markdown noise and angle-tag remnants\n",
    "        txt = re.sub(r\"[`*_<>\\[\\]#]{1,}\", \" \", txt)\n",
    "        # collapse runs of punctuation/spaces\n",
    "        txt = re.sub(r\"\\s*[.,;:!?]\\s*\", lambda m: m.group(0).strip() + \" \", txt)\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        # keep the first sentence or ~30 words\n",
    "        sent = _first_sentence(txt)\n",
    "        if not sent:\n",
    "            parts = txt.split()\n",
    "            sent = \" \".join(parts[:30])\n",
    "        return sent[:400].strip()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_code_ce_acc(\n",
    "        model: \"AgenticTransformerSeq2Seq\",\n",
    "        X: torch.Tensor,\n",
    "        Y: torch.Tensor,\n",
    "        *,\n",
    "        device: str = DEVICE\n",
    "        ) -> Tuple[float, float]:\n",
    "        \"\"\"Teacher-forced CE/accuracy for the Code agent on input X vs gold Y.\"\"\"\n",
    "        model.to(device); model.eval()\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "        y_in, y_tgt = shift_targets(Y.to(device))\n",
    "        logits = model.forward_role(X.to(device), y_in, agent_id=AGENT_CODE_GENERATION)\n",
    "        ce = float(loss_fn(logits, y_tgt).item())\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = (y_tgt != model.pad_idx)\n",
    "        acc = float((((preds == y_tgt) & mask).float().sum() / (mask.float().sum() + 1e-8)).item())\n",
    "        return ce, acc\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_pipeline_lift(\n",
    "        model: AgenticTransformerSeq2Seq,\n",
    "        tok: SubwordTokenizer,\n",
    "        X: torch.Tensor,\n",
    "        Y: torch.Tensor,\n",
    "        *,\n",
    "        issue_max_len: int,\n",
    "        max_in_len: int,\n",
    "        device: str = DEVICE\n",
    "    ):\n",
    "        model.to(device); model.eval()\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "\n",
    "        # 1) CE with **no issue text** to CODE (true baseline).\n",
    "        #    Use a single UNK token as minimal, non-masked encoder input to avoid all-pad attention edge cases.\n",
    "        y_in, y_tgt = shift_targets(Y.to(device))\n",
    "        B = X.size(0)\n",
    "        X_no_issue = torch.full((B, 1), UNK, dtype=torch.long, device=device)  # shape [B, 1]\n",
    "        logits_base = model.forward_role(X_no_issue, y_in, agent_id=AGENT_CODE_GENERATION)\n",
    "        ce_base = float(loss_fn(logits_base, y_tgt).item())\n",
    "        acc_base = float((((logits_base.argmax(-1) == y_tgt) & (y_tgt != model.pad_idx)).float().sum())\n",
    "                         / ((y_tgt != model.pad_idx).float().sum().clamp_min(1.0)))\n",
    "\n",
    "        # 2) CE with **with gist** context to CODE\n",
    "        issue_ctx, _ = _issue_ctx_greedy_with_fallback(model, tok, X.to(device), issue_max_len=issue_max_len)\n",
    "        gist_only = issue_ctx[:, :max_in_len]\n",
    "        logits_gist = model.forward_role(gist_only, y_in, agent_id=AGENT_CODE_GENERATION)\n",
    "        ce_gist = float(loss_fn(logits_gist, y_tgt).item())\n",
    "        acc_gist = float((((logits_gist.argmax(-1) == y_tgt) & (y_tgt != model.pad_idx)).float().sum())\n",
    "                         / ((y_tgt != model.pad_idx).float().sum().clamp_min(1.0)))\n",
    "\n",
    "        print(\"\\n[Agentic][Testing][PIPELINE-LIFT] Teacher-forced delta (with gist vs **no-issue baseline**; more negative is better)\")\n",
    "        print(f\"[Agentic][Testing][PIPELINE-LIFT] CODE CE(no-issue)={ce_base:.3f} | CE(with gist)={ce_gist:.3f} | ΔCE={ce_gist - ce_base:.3f} | acc(no-issue)={acc_base:.3f} | acc(with gist)={acc_gist:.3f}\")\n",
    "    # ============================================================\n",
    "    # Diagnostics / reporting\n",
    "    # ============================================================\n",
    "    @torch.no_grad()\n",
    "    def _print_agent_role_outputs_after(model: AgenticTransformerSeq2Seq, X: torch.Tensor, y: torch.Tensor,\n",
    "                                        *, n_tokens: int = 3, n_agents: Optional[int] = None,\n",
    "                                        device: str = DEVICE) -> None:\n",
    "        \"\"\"Debug helper: prints a small slice of role-head logits for each agent on the last decode step.\"\"\"\n",
    "        model.to(device); model.eval()\n",
    "        nA = len(model.routing.agents) if n_agents is None else n_agents\n",
    "        y_in, _ = shift_targets(y.to(device))\n",
    "        mem, cls, src_mask = model.encode(X.to(device))\n",
    "        dec_states = model.decode_states(y_in, mem, src_mask)\n",
    "        last = dec_states[:, -1:]\n",
    "        for a in range(nA):\n",
    "            logits = model.routing.agents[a].project(last, head=\"role\")\n",
    "            vec = logits[0, 0, :n_tokens].detach().cpu().numpy()\n",
    "            print(f\"[{agent_pretty_name(a)}] role_head logits[:{n_tokens}] -> {vec}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def per_agent_role_eval_code_on_gist(model, tok, X, Y, *, issue_max_len: int, max_in_len: int, device: str = DEVICE):\n",
    "        model.to(device); model.eval()\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "        y_in, y_tgt = shift_targets(Y.to(device))\n",
    "        issue_ctx, _ = _issue_ctx_greedy_with_fallback(model, tok, X.to(device), issue_max_len=issue_max_len)\n",
    "        gist_only = issue_ctx[:, :max_in_len]\n",
    "        logits = model.forward_role(gist_only, y_in, agent_id=AGENT_CODE_GENERATION)\n",
    "        ce = float(loss_fn(logits, y_tgt).item())\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = (y_tgt != model.pad_idx)\n",
    "        acc = float((((preds == y_tgt) & mask).float().sum() / mask.float().sum().clamp_min(1.0)).item())\n",
    "        print(f\"[Agentic][Testing][CODE@GIST] CE={ce:.3f} | tok_acc={acc:.3f} | N={int(X.size(0))}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Small tensor helpers\n",
    "    # ============================================================\n",
    "    def _concat_truncate(a: torch.Tensor, b: torch.Tensor, *, max_len: int) -> torch.Tensor:\n",
    "        out = torch.cat([a, b], dim=1)\n",
    "        if out.size(1) > max_len:\n",
    "            out = out[:, :max_len]\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def issue_analysis_stats(model: AgenticTransformerSeq2Seq, tok: SubwordTokenizer, X: torch.Tensor,\n",
    "                            *, issue_max_len: int, device: str = DEVICE) -> Dict[str, float]:\n",
    "        \"\"\"Quick quality probes on A's output: length, lines, and crude code leakage.\"\"\"\n",
    "        model.to(device); model.eval()\n",
    "        issues = _generate_static(model, X.to(device), agent_id=AGENT_ISSUE_ANALYSIS,\n",
    "            max_len=issue_max_len, top_k=20, top_p=0.90,\n",
    "            temperature=0.8, no_repeat_ngram_size=4, min_len=24  # Adjusted min_len\n",
    "    )\n",
    "        ISSUE = min(4, issues.size(0))\n",
    "        decoded = [tok.decode([t for t in issues[i].tolist() if t not in (tok.pad, tok.bos, tok.eos)]) for i in range(ISSUE)]\n",
    "        lengths = [len(s.split()) for s in decoded]\n",
    "        line_counts = [s.count(\"\\n\") + 1 for s in decoded]\n",
    "        code_leak_lines = sum(1 for s in decoded for ln in s.splitlines()\n",
    "                              if (\"diff --git\" in ln) or (\"def \" in ln) or ln.strip().startswith(\"class \") or (\"```\" in ln))\n",
    "        return {\n",
    "            \"sampled\": float(ISSUE),\n",
    "            \"avg_tokens\": float(np.mean(lengths) if lengths else 0.0),\n",
    "            \"avg_lines\": float(np.mean(line_counts) if line_counts else 0.0),\n",
    "            \"code_leak_lines\": float(code_leak_lines),\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def issue_to_code_alignment_sample(model: AgenticTransformerSeq2Seq, tok: SubwordTokenizer, X: torch.Tensor,\n",
    "                                      *, issue_max_len: int, out_max_len: int, max_in_len: int, k: int = 3,\n",
    "                                      device: str = DEVICE) -> None:\n",
    "        \"\"\"Print K examples: issue (A) and patch (B) to eyeball alignment.\"\"\"\n",
    "        model.to(device); model.eval()\n",
    "        Xk = X[:k].to(device)\n",
    "        issue_ids, patch_ids = model.routing.run_pipeline(\n",
    "            model, tok, Xk, issue_max_len=issue_max_len, out_max_len=out_max_len, max_in_len=max_in_len\n",
    "        )\n",
    "        for i in range(min(k, Xk.size(0))):\n",
    "            issue = tok.decode([t for t in issue_ids[i].tolist() if t not in (tok.pad, tok.bos, tok.eos)])\n",
    "            patch = tok.decode([t for t in patch_ids[i].tolist() if t not in (tok.pad, tok.bos, tok.eos)])\n",
    "            print(f\"\\n=== Example {i} ===\")\n",
    "            print(\"[ISSUE]\\n\", issue[:800])\n",
    "            print(\"\\n[PATCH]\\n\", patch[:800])\n",
    "\n",
    "    def _first_sentence(txt: str) -> str:\n",
    "        # crude first sentence splitter; falls back to first ~30 words\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        m = re.search(r\"(.+?[.!?])(\\s|$)\", txt)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        # fallback: ~30 words\n",
    "        parts = txt.split()\n",
    "        return \" \".join(parts[:30]) if parts else \"\"\n",
    "\n",
    "    def _first_n_words(txt: str, n: int = 100) -> str:\n",
    "        parts = txt.split()[:n]\n",
    "        return ' '.join(parts)\n",
    "\n",
    "    def _first_k_sentences(txt: str, k: int = 3) -> str:\n",
    "        # Split on sentence boundaries, preserving punctuation\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', txt)[:k]\n",
    "        return ' '.join(sentences).strip()\n",
    "\n",
    "    def _make_issue_gist(title: str, desc: str) -> str:\n",
    "        title = (title or \"\").strip()\n",
    "        desc = (desc or \"\").strip()\n",
    "        if not title and not desc:\n",
    "            return \"\"\n",
    "        # core = _first_n_words(desc, 100) if desc else \"\"  # Alternative: word-based\n",
    "        core = _first_k_sentences(desc, 3) if desc else \"\"  # Sentence-based for coherence\n",
    "        if title and core:\n",
    "            return f\"{title}: {core}\"\n",
    "        return title or core\n",
    "\n",
    "    def _clean_issue_text(txt: str) -> str:\n",
    "        # keep simple printable range; strip emojis/control chars\n",
    "        return re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E]\", \"\", txt).strip()\n",
    "\n",
    "    def _postprocess_gist(txt: str) -> str:\n",
    "        # Strong cleanup for display + context\n",
    "        txt = _clean_issue_text(txt)\n",
    "        # Remove backticks/markdown noise and angle-tag remnants\n",
    "        txt = re.sub(r\"[`*_<>\\[\\]#]{1,}\", \" \", txt)\n",
    "        # Collapse runs of punctuation/spaces\n",
    "        txt = re.sub(r\"\\s*[.,;:!?]\\s*\", lambda m: m.group(0).strip() + \" \", txt)\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        # Keep first 3 sentences or ~100 words\n",
    "        # sent = _first_n_words(txt, 100)  # Alternative: word-based\n",
    "        sent = _first_k_sentences(txt, 3)\n",
    "        if not sent:\n",
    "            parts = txt.split()\n",
    "            sent = \" \".join(parts[:100])\n",
    "        return sent.strip()  # Removed [:400] cap\n",
    "\n",
    "    def _is_noisy_gist(txt: str) -> bool:\n",
    "        if not txt:\n",
    "            return True\n",
    "\n",
    "        # Token and character level sanity\n",
    "        words = txt.split()\n",
    "        if len(words) < 6:                      # was 4; require a bit more substance\n",
    "            return True\n",
    "\n",
    "        # Alphanumeric density: require at least 30% of characters to be [A-Za-z0-9]\n",
    "        alnum = sum(ch.isalnum() for ch in txt)\n",
    "        if (alnum / max(len(txt), 1)) < 0.30:\n",
    "            return True\n",
    "\n",
    "        # Disallow obvious code/patch markers\n",
    "        bad_markers = (\n",
    "            \"diff --git\", \"```\", \"@@\", \"+++\", \"---\",\n",
    "            \"class \", \"def \", \"://\", \"/pytorch\", \"/prefect\"\n",
    "        )\n",
    "        if any(b in txt for b in bad_markers):\n",
    "            return True\n",
    "\n",
    "        # Too many non-word symbols (count everything except letters, digits, and spaces)\n",
    "        nonword = re.sub(r\"[A-Za-z0-9\\s]\", \"\", txt)\n",
    "        if (len(nonword) / max(len(txt), 1)) >= 0.35:   # use >= and higher threshold\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _decode_row_no_pad(tok: \"SubwordTokenizer\", row: torch.Tensor) -> str:\n",
    "        ids = [int(t) for t in row.tolist() if int(t) != tok.pad]\n",
    "        return tok.decode(ids)\n",
    "\n",
    "    def _fallback_gist_from_input_text(in_text: str) -> str:\n",
    "        title = _extract_tag_block(in_text, \"ISSUE_TITLE\")\n",
    "        desc  = _extract_tag_block(in_text, \"ISSUE_DESC\") or in_text\n",
    "        return _postprocess_gist(_make_issue_gist(title, desc))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _issue_ctx_greedy_with_fallback(\n",
    "        model: \"AgenticTransformerSeq2Seq\",\n",
    "        tok: \"SubwordTokenizer\",\n",
    "        X: torch.Tensor,\n",
    "        *,\n",
    "        issue_max_len: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          issue_ctx  : [B, Tctx] token ids for <ISSUE_GIST>...</ISSUE_GIST> (no BOS/EOS)\n",
    "          display_ids: [B, Tdisp] BOS ... EOS ids of the plain gist for printing\n",
    "        \"\"\"\n",
    "        # 1) Greedy generation (no sampling) from the Issue agent\n",
    "        gen_ids = _generate_static(\n",
    "            model, X, agent_id=AGENT_ISSUE_ANALYSIS, max_len=issue_max_len,\n",
    "            top_k=None, top_p=None, temperature=1.0, no_repeat_ngram_size=3, min_len=24  # Increased min_len\n",
    "        )\n",
    "\n",
    "        B = X.size(0)\n",
    "        gists: List[str] = []\n",
    "\n",
    "        # 2) Per-row cleanup + fallback to deterministic gist if noisy\n",
    "        for i in range(B):\n",
    "            raw = [t for t in gen_ids[i].tolist() if t not in (tok.pad, tok.bos, tok.eos)]\n",
    "            gen_txt = tok.decode(raw)\n",
    "            gen_txt = _postprocess_gist(gen_txt)\n",
    "\n",
    "            # First-level check\n",
    "            if _is_noisy_gist(gen_txt):\n",
    "                in_text = _decode_row_no_pad(tok, X[i])\n",
    "                gen_txt = _fallback_gist_from_input_text(in_text)\n",
    "                gen_txt = _postprocess_gist(gen_txt)\n",
    "\n",
    "            # Final safety: if still noisy, force a minimal title-only fallback\n",
    "            if _is_noisy_gist(gen_txt):\n",
    "                in_text = _decode_row_no_pad(tok, X[i])\n",
    "                title = _extract_tag_block(in_text, \"ISSUE_TITLE\")\n",
    "                gen_txt = (title or \"Issue: (no description)\").strip()\n",
    "\n",
    "            gists.append(gen_txt or \"Issue: (no description)\")\n",
    "\n",
    "        # 3) Encode context (<ISSUE_GIST>…</ISSUE_GIST>) and plain display ids\n",
    "        ctx_rows, disp_rows = [], []\n",
    "        for g in gists:\n",
    "            ctx_txt = f\"<ISSUE_GIST>\\n{g}\\n</ISSUE_GIST>\"\n",
    "            ctx_rows.append(torch.tensor(tok.sp.encode(ctx_txt, out_type=int), dtype=torch.long))\n",
    "            disp_rows.append(tok.encode(g, add_bos_eos=True, max_len=issue_max_len))\n",
    "\n",
    "        issue_ctx   = pad_sequence(ctx_rows, batch_first=True, padding_value=tok.pad)\n",
    "        display_ids = pad_sequence(disp_rows, batch_first=True, padding_value=tok.pad)\n",
    "        return issue_ctx, display_ids\n",
    "\n",
    "    # === NEW: helper to precompute gist-only encoder inputs =======================\n",
    "    @torch.no_grad()\n",
    "    def build_gist_only_inputs(\n",
    "        model: \"AgenticTransformerSeq2Seq\",\n",
    "        tok: \"SubwordTokenizer\",\n",
    "        X: torch.Tensor,\n",
    "        *,\n",
    "        issue_max_len: int,\n",
    "        max_in_len: int,\n",
    "        device: str = DEVICE\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns encoder inputs that contain ONLY <ISSUE_GIST>…</ISSUE_GIST>,\n",
    "        truncated to max_in_len. This matches how the Code agent is served.\n",
    "        \"\"\"\n",
    "        model.to(device); model.eval()\n",
    "        issue_ctx, _ = _issue_ctx_greedy_with_fallback(\n",
    "            model, tok, X.to(device), issue_max_len=issue_max_len\n",
    "        )\n",
    "        gist_only = issue_ctx[:, :max_in_len]\n",
    "        return gist_only\n",
    "\n",
    "    def _unfreeze_decoder_tail(model: AgenticTransformerSeq2Seq, n_last_blocks: int = 1):\n",
    "        # Unfreeze final N transformer decoder layers + all decoder LayerNorms\n",
    "        if hasattr(model.decoder, \"decoder\"):\n",
    "            # PyTorch TransformerDecoder with 'layers'\n",
    "            layers = getattr(model.decoder.decoder, \"layers\", [])\n",
    "        else:\n",
    "            layers = []\n",
    "        # Unfreeze norms everywhere in decoder\n",
    "        for name, p in model.decoder.named_parameters():\n",
    "            if \"norm\" in name:\n",
    "                p.requires_grad = True\n",
    "        # Unfreeze last N full blocks\n",
    "        if layers:\n",
    "            for bl in layers[-n_last_blocks:]:\n",
    "                for p in bl.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "    # =========================\n",
    "    # Issue agent: evaluation\n",
    "    # =========================\n",
    "    @torch.no_grad()\n",
    "    def _eval_issue_ce_acc(\n",
    "        model: \"AgenticTransformerSeq2Seq\",\n",
    "        X: torch.Tensor,\n",
    "        P: torch.Tensor,\n",
    "        *,\n",
    "        device: str = DEVICE\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Teacher-forced CE/accuracy for the Issue agent on input X vs gold ISSUE_DESC targets P.\n",
    "        \"\"\"\n",
    "        model.to(device); model.eval()\n",
    "        loss_fn = SeqCELoss(pad_idx=model.pad_idx)\n",
    "        y_in, y_tgt = shift_targets(P.to(device))\n",
    "        logits = model.forward_role(X.to(device), y_in, agent_id=AGENT_ISSUE_ANALYSIS)\n",
    "        ce = float(loss_fn(logits, y_tgt).item())\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = (y_tgt != model.pad_idx)\n",
    "        acc = float((((preds == y_tgt) & mask).float().sum() / mask.float().sum().clamp_min(1.0)).item())\n",
    "        return ce, acc\n",
    "\n",
    "    # ============================================================\n",
    "    # Orchestration\n",
    "    # ============================================================\n",
    "    def run_all(cfg: Config = CFG):\n",
    "        set_seed(cfg.seed)\n",
    "\n",
    "        # Data\n",
    "        data = SWEText2PatchData(split=\"train\", limit=cfg.limit, max_in_len=cfg.max_in_len,\n",
    "                                 max_out_len=cfg.max_out_len, spm_vocab_size=cfg.spm_vocab,\n",
    "                                 demo_data=cfg.demo_data)\n",
    "\n",
    "        # Issue targets = ISSUE_DESC\n",
    "        ids, X, Y, P = data.as_tensors_with_issue_targets(issue_max_len=min(cfg.max_out_len, 256))\n",
    "\n",
    "        # Deterministic shuffle then split\n",
    "        N = len(ids)\n",
    "        g = torch.Generator().manual_seed(cfg.seed)\n",
    "        perm = torch.randperm(N, generator=g)\n",
    "        ids = [ids[i] for i in perm.tolist()]\n",
    "        X, Y, P = X[perm], Y[perm], P[perm]\n",
    "        split = int(N * 0.8)\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        Y_train, Y_test = Y[:split], Y[split:]\n",
    "        P_train, P_test = P[:split], P[split:]\n",
    "        print(f\"[Info] Train: {split} pairs, Test: {N - split} pairs\")\n",
    "\n",
    "        # Model\n",
    "        max_len_for_model = max(cfg.max_len_cap, X.size(1) + min(cfg.max_out_len, 256))\n",
    "        model = AgenticTransformerSeq2Seq(\n",
    "            vocab_size=data.tok.vocab_size,\n",
    "            n_agents=cfg.n_agents, model_dim=cfg.model_dim, n_heads=cfg.n_heads,\n",
    "            n_layers_enc=cfg.n_layers_enc, n_layers_dec=cfg.n_layers_dec,\n",
    "            max_len=max_len_for_model, pad_idx=data.tok.pad\n",
    "        )\n",
    "\n",
    "        # ===== Stage 1: Interleaved ISSUE↔CODE =====\n",
    "        print(\"[Agentic][Training] Stage 1: Interleaved ISSUE↔CODE (same epoch)\")\n",
    "        train_stage1_interleaved(\n",
    "            model, X_train, Y_train, P_train,\n",
    "            tok=data.tok,\n",
    "            issue_max_len=min(cfg.max_out_len, 256),\n",
    "            epochs=cfg.pipe_epochs,\n",
    "            batch_size=cfg.pipe_batch,\n",
    "            lr=cfg.pipe_lr,\n",
    "            device=DEVICE,\n",
    "            unfreeze_backbone=True,\n",
    "            unfreeze_adapters=cfg.ft_unfreeze_adapters,\n",
    "            unfreeze_dec_norms=cfg.ft_unfreeze_dec_norms,\n",
    "            max_in_len=cfg.max_in_len\n",
    "        )\n",
    "\n",
    "        # ------ Pipeline Lift (teacher-forced) ------\n",
    "        eval_pipeline_lift(\n",
    "            model, data.tok, X_test, Y_test,\n",
    "            issue_max_len=min(cfg.max_out_len, 256),\n",
    "            max_in_len=cfg.max_in_len,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # NEW: evaluate Code agent on gist-only input (teacher-forced)\n",
    "        per_agent_role_eval_code_on_gist(\n",
    "            model, data.tok, X_test, Y_test,\n",
    "            issue_max_len=min(cfg.max_out_len, 256),\n",
    "            max_in_len=cfg.max_in_len,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # ------ Diagnostics ------\n",
    "        print(\"\\n[Agentic][Testing][ISSUE-STATS] Analysis agent quick stats (first few issues)\")\n",
    "        stats = issue_analysis_stats(model, data.tok, X_test, issue_max_len=min(cfg.max_out_len, 256), device=DEVICE)\n",
    "        print(stats)\n",
    "\n",
    "        print(\"\\n[Agentic][Testing][PIPELINE-SAMPLES] Issue ↔ Patch examples (eyeball alignment)\")\n",
    "        issue_to_code_alignment_sample(model, data.tok, X_test,\n",
    "                                       issue_max_len=min(cfg.max_out_len, 256),\n",
    "                                       out_max_len=cfg.decode_max_len,\n",
    "                                       max_in_len=cfg.max_in_len,\n",
    "                                       k=3, device=DEVICE)\n",
    "\n",
    "        # ===== Helpers for CE/Acc =====\n",
    "        @torch.no_grad()\n",
    "        def _eval_issue_ce_acc_local(m: AgenticTransformerSeq2Seq, Xenc: torch.Tensor, Ptg: torch.Tensor, *, device: str):\n",
    "            m.to(device); m.eval()\n",
    "            loss_fn = SeqCELoss(pad_idx=m.pad_idx)\n",
    "            y_in, y_tgt = shift_targets(Ptg.to(device))\n",
    "            logits = m.forward_role(Xenc.to(device), y_in, agent_id=AGENT_ISSUE_ANALYSIS)\n",
    "            ce = float(loss_fn(logits, y_tgt).item())\n",
    "            preds = logits.argmax(-1)\n",
    "            mask = (y_tgt != m.pad_idx)\n",
    "            acc = float((((preds == y_tgt) & mask).float().sum() / mask.float().sum().clamp_min(1.0)).item())\n",
    "            return ce, acc\n",
    "\n",
    "        # ===========================\n",
    "        # ===== Stage 2A: Issue FT\n",
    "        # ===========================\n",
    "        print(\"\\n[Agentic][Training] Stage 2A: Static specialization for ISSUE agent \"\n",
    "              \"(freeze backbone + Code agent; train Issue agent on original X with P targets)\")\n",
    "\n",
    "        iss_ce_before, iss_acc_before = _eval_issue_ce_acc_local(model, X_test, P_test, device=DEVICE)\n",
    "        print(f\"[Agentic][Testing][ISSUE@Before FT] CE={iss_ce_before:.3f} | tok_acc={iss_acc_before:.3f}\")\n",
    "\n",
    "        fine_tune_static(\n",
    "            model, X_train, Y_train,                 # Y_train ignored for Issue FT; P_train used as targets\n",
    "            user_id=AGENT_ISSUE_ANALYSIS,\n",
    "            epochs=cfg.ft_epochs,\n",
    "            batch_size=cfg.ft_batch,\n",
    "            lr=cfg.ft_lr,\n",
    "            weight_decay=0.01,\n",
    "            unfreeze_adapters=cfg.ft_unfreeze_adapters,\n",
    "            unfreeze_dec_norms=cfg.ft_unfreeze_dec_norms, # unfreeze decoder norms frozen if you want tighter freeze\n",
    "            unfreeze_decoder_tail_blocks=0,          # no extra decoder capacity needed for Issue FT\n",
    "            idxs=None,\n",
    "            device=DEVICE,\n",
    "            tok=data.tok,\n",
    "            P=P_train,                               # REQUIRED for Issue FT\n",
    "            gist_ctx_fn=None,                        # not used for Issue FT\n",
    "            max_in_len=CFG.max_in_len,\n",
    "            use_concat_first_epoch=False,            # not applicable to Issue FT\n",
    "            patience=2\n",
    "        )\n",
    "\n",
    "        iss_ce_after, iss_acc_after = _eval_issue_ce_acc_local(model, X_test, P_test, device=DEVICE)\n",
    "        print(\n",
    "            f\"[Agentic][Testing][ISSUE@After FT] \"\n",
    "            f\"CE={iss_ce_after:.3f} | tok_acc={iss_acc_after:.3f} \"\n",
    "            f\"| ΔCE={iss_ce_after - iss_ce_before:+.3f} ({(iss_ce_after - iss_ce_before) / iss_ce_before * 100:+.2f}%) \"\n",
    "            f\"| Δacc={iss_acc_after - iss_acc_before:+.3f} ({(iss_acc_after - iss_acc_before) / iss_acc_before * 100:+.2f}%)\"\n",
    "        )\n",
    "\n",
    "        # ===========================\n",
    "        # ===== Stage 2B: Code FT\n",
    "        # ===========================\n",
    "        print(\"\\n[Agentic][Training] Stage 2B: Static specialization for CODE agent \"\n",
    "              \"(freeze backbone + Issue agent; train Code agent on gist-only)\")\n",
    "\n",
    "        # Build gist-only inputs (once; no grad) for test measurement\n",
    "        issue_len = min(cfg.max_out_len, 256)\n",
    "        X_test_gist = build_gist_only_inputs(\n",
    "            model, data.tok, X_test,\n",
    "            issue_max_len=issue_len, max_in_len=cfg.max_in_len, device=DEVICE\n",
    "        )\n",
    "\n",
    "        ce_before, acc_before = _eval_code_ce_acc(model, X_test_gist, Y_test, device=DEVICE)\n",
    "        print(f\"[Agentic][Testing][CODE@GIST][Before FT] CE={ce_before:.3f} | tok_acc={acc_before:.3f}\")\n",
    "\n",
    "        def _gist_ctx_fn_for_ft(xb_device):\n",
    "            issue_ctx, _ = _issue_ctx_greedy_with_fallback(model, data.tok, xb_device, issue_max_len=min(CFG.max_out_len, 256))\n",
    "            return issue_ctx  # gist-only\n",
    "\n",
    "        fine_tune_static(\n",
    "            model, X_train, Y_train,\n",
    "            user_id=AGENT_CODE_GENERATION,\n",
    "            epochs=cfg.ft_epochs,\n",
    "            batch_size=cfg.ft_batch,\n",
    "            lr=cfg.ft_lr,\n",
    "            weight_decay=0.01,\n",
    "            unfreeze_adapters=cfg.ft_unfreeze_adapters,\n",
    "            unfreeze_dec_norms=cfg.ft_unfreeze_dec_norms,\n",
    "            unfreeze_decoder_tail_blocks=1,\n",
    "            idxs=None,                      # or use a modulo-slice if you want per-agent shards\n",
    "            device=DEVICE,\n",
    "            tok=data.tok,\n",
    "            P=P_train,                      # used only for epoch-1 curriculum (clean gist)\n",
    "            gist_ctx_fn=_gist_ctx_fn_for_ft,\n",
    "            max_in_len=CFG.max_in_len,\n",
    "            use_concat_first_epoch=True,\n",
    "            patience=2\n",
    "        )\n",
    "\n",
    "        # Measure CODE@GIST after FT (recompute gist from current Issue agent)\n",
    "        ce_after, acc_after = _eval_code_ce_acc(\n",
    "            model,\n",
    "            _gist_ctx_fn_for_ft(X_test.to(DEVICE))[:, :CFG.max_in_len],\n",
    "            Y_test,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        print(\n",
    "            f\"[Agentic][Testing][CODE@GIST][After FT] \"\n",
    "            f\"CE={ce_after:.3f} | tok_acc={acc_after:.3f} \"\n",
    "            f\"| ΔCE={ce_after - ce_before:+.3f} ({(ce_after - ce_before) / ce_before * 100:+.2f}%) \"\n",
    "            f\"| Δacc={acc_after - acc_before:+.3f} ({(acc_after - acc_before) / acc_before * 100:+.2f}%)\"\n",
    "        )\n",
    "\n",
    "        return model, data, (ids, X, Y, P)\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        model, data, tensors = run_all(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fa15c-91ae-4f8c-a566-bf45b1dd457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
